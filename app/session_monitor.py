"""
Background session monitor for InnerBoard recording.

This module tails a growing typescript log and its corresponding timing file
generated by the `script` utility and performs the following duties:

- Detects user inactivity gaps of a configured duration based on the timing file
  (util-linux and BSD formats both emit lines like: "<delta_seconds> <num_bytes>")
- Detects when the console log exceeds a configured number of lines since the
  last processed cut
- For each trigger, extracts the raw segment (log bytes and timing lines), stores
  artifacts under a per-session segments directory, cleans the log, generates SRE
  JSON using the AdviceService, and updates processing state
- Defers physical deletion (compaction) of processed prefixes of the live log and
  timing files until the recording process exits, to avoid disrupting the writer
"""

from __future__ import annotations

import io
import json
import os
import threading
import time
from dataclasses import dataclass, asdict
from pathlib import Path
from typing import List, Optional, Tuple

from app.utils import ensure_directory, clean_terminal_log
from app.llm import LocalLLM
from app.advice import AdviceService
from app.safety import no_network
from app.config import config
from app.logging_config import get_logger


logger = get_logger(__name__)


@dataclass
class ProcessingState:
    """Tracks how much of the typescript has been processed/archived.

    Attributes:
        processed_log_bytes: Number of bytes consumed from the start of the log file
        processed_timing_lines: Number of lines consumed from the start of the timing file
        next_segment_index: Monotonic index for segment naming
    """

    processed_log_bytes: int = 0
    processed_timing_lines: int = 0
    next_segment_index: int = 1


def _read_new_timing_lines(timing_path: Path, start_line_index: int) -> Tuple[List[Tuple[float, int]], int]:
    """Read and parse timing lines from `start_line_index` to EOF.

    Returns a list of (delta_seconds, byte_count) and the new total line count.
    """
    lines: List[str] = []
    try:
        with timing_path.open("r", encoding="utf-8", errors="ignore") as fp:
            lines = fp.read().splitlines()
    except FileNotFoundError:
        return [], start_line_index
    parsed: List[Tuple[float, int]] = []
    for line in lines[start_line_index:]:
        parts = line.strip().split()
        if len(parts) >= 2:
            try:
                delta = float(parts[0])
                num_bytes = int(float(parts[1]))
                parsed.append((delta, num_bytes))
            except ValueError:
                continue
    return parsed, len(lines)


def _count_log_lines_and_offsets(log_path: Path, start_byte_offset: int, max_lines: int) -> Tuple[int, int]:
    """Count lines from `start_byte_offset` up to at most `max_lines`.

    Returns (num_lines_found, byte_offset_after_counted_lines).
    If fewer than max_lines are present, returns EOF offset.
    """
    try:
        with log_path.open("rb") as fp:
            fp.seek(start_byte_offset)
            num_lines = 0
            offset = start_byte_offset
            chunk_size = 1024 * 64
            while True:
                chunk = fp.read(chunk_size)
                if not chunk:
                    return num_lines, fp.tell()
                offset_end = offset + len(chunk)
                # Count newlines
                newlines = chunk.count(b"\n")
                if num_lines + newlines >= max_lines:
                    # Find the exact byte where the Nth newline occurs
                    needed = max_lines - num_lines
                    idx = -1
                    start = 0
                    for _ in range(needed):
                        idx = chunk.find(b"\n", start)
                        if idx == -1:
                            break
                        start = idx + 1
                    if idx == -1:
                        # Should not happen given the count; fall back to end
                        return num_lines + newlines, offset_end
                    return max_lines, offset + idx + 1
                num_lines += newlines
                offset = offset_end
    except FileNotFoundError:
        return 0, start_byte_offset


def _write_bytes_to_file(target_path: Path, data: bytes) -> None:
    target_path.parent.mkdir(parents=True, exist_ok=True)
    with target_path.open("wb") as fp:
        fp.write(data)


def _write_lines_to_file(target_path: Path, lines: List[str]) -> None:
    target_path.parent.mkdir(parents=True, exist_ok=True)
    with target_path.open("w", encoding="utf-8") as fp:
        fp.write("\n".join(lines) + ("\n" if lines else ""))


class SessionMonitor(threading.Thread):
    """Monitor a live typescript `.log` and `.timing` pair and segment them.

    The monitor runs in a dedicated daemon thread. It is designed to be started
    immediately after the recorder process starts and joined after it exits.
    """

    def __init__(
        self,
        log_path: Path,
        timing_path: Path,
        session_root_dir: Optional[Path] = None,
        inactivity_seconds: int = 15 * 60,
        max_lines_per_segment: int = 1000,
        llm_model: Optional[str] = None,
    ) -> None:
        super().__init__(name="InnerBoardSessionMonitor", daemon=True)
        self.log_path = Path(log_path)
        self.timing_path = Path(timing_path)
        self.session_root_dir = (
            Path(session_root_dir) if session_root_dir else self.log_path.parent
        )
        self.inactivity_seconds = inactivity_seconds
        self.max_lines_per_segment = max_lines_per_segment
        self.llm_model = llm_model

        self._stop_event = threading.Event()

        stem = self.log_path.stem  # e.g., session_...
        # Directory to store segments and state
        self.session_dir = ensure_directory(self.session_root_dir / stem)
        self.segments_dir = ensure_directory(self.session_dir / "segments")
        self.state_path = self.session_dir / "state.json"
        self.state = self._load_state()

    def _load_state(self) -> ProcessingState:
        if self.state_path.exists():
            try:
                with self.state_path.open("r", encoding="utf-8") as fp:
                    data = json.load(fp)
                return ProcessingState(**data)
            except Exception as e:
                logger.warning(f"Failed to load monitor state: {e}")
        return ProcessingState()

    def _save_state(self) -> None:
        try:
            with self.state_path.open("w", encoding="utf-8") as fp:
                json.dump(asdict(self.state), fp, indent=2)
        except Exception as e:
            logger.warning(f"Failed to save monitor state: {e}")

    def stop(self) -> None:
        self._stop_event.set()

    def run(self) -> None:
        logger.debug(
            f"SessionMonitor started for log={self.log_path} timing={self.timing_path}"
        )
        # Main polling loop
        while not self._stop_event.is_set():
            try:
                # 1) Check for inactivity boundary in new timing lines
                timing_entries, total_lines = _read_new_timing_lines(
                    self.timing_path, self.state.processed_timing_lines
                )
                inactivity_trigger_index = None
                bytes_until_inactivity = 0
                for idx, (delta_s, num_bytes) in enumerate(timing_entries):
                    if delta_s >= float(self.inactivity_seconds):
                        inactivity_trigger_index = idx
                        break
                    bytes_until_inactivity += num_bytes

                # 2) Check size threshold in the log since last processed
                num_lines, line_threshold_end_offset = _count_log_lines_and_offsets(
                    self.log_path, self.state.processed_log_bytes, self.max_lines_per_segment
                )

                should_cut_for_inactivity = inactivity_trigger_index is not None and bytes_until_inactivity > 0
                should_cut_for_size = num_lines >= self.max_lines_per_segment

                if should_cut_for_inactivity or should_cut_for_size:
                    if should_cut_for_inactivity:
                        segment_log_bytes = bytes_until_inactivity
                        segment_timing_lines_count = inactivity_trigger_index  # exclude the gap line
                    else:
                        # Determine timing lines that cover the bytes up to the 1000-line end offset
                        target_bytes = max(0, line_threshold_end_offset - self.state.processed_log_bytes)
                        segment_log_bytes = 0
                        segment_timing_lines_count = 0
                        for idx, (_delta, num_bytes) in enumerate(timing_entries):
                            segment_log_bytes += num_bytes
                            segment_timing_lines_count = idx + 1
                            if segment_log_bytes >= target_bytes:
                                break

                    if segment_log_bytes > 0 and segment_timing_lines_count >= 0:
                        self._process_and_archive_segment(
                            segment_log_bytes=segment_log_bytes,
                            segment_timing_lines=segment_timing_lines_count,
                            total_timing_lines_after_read=total_lines,
                        )
                # Sleep a bit before polling again
                time.sleep(2.0)
            except Exception as e:
                logger.warning(f"SessionMonitor loop error: {e}")
                time.sleep(3.0)

        logger.debug("SessionMonitor stopping (stop requested)")

    def _process_and_archive_segment(
        self,
        segment_log_bytes: int,
        segment_timing_lines: int,
        total_timing_lines_after_read: int,
    ) -> None:
        """Extract segment, generate SRE, and advance state.

        This function copies the raw segment bytes/lines into a dedicated segment
        folder, generates SRE JSON from the cleaned log, and advances the
        processed offsets. Physical deletion of consumed prefixes is deferred.
        """
        # Compute segment paths
        seg_index = self.state.next_segment_index
        seg_dir = ensure_directory(self.segments_dir / f"segment_{seg_index:04d}")
        raw_log_path = seg_dir / "raw.log"
        raw_timing_path = seg_dir / "segment.timing"
        cleaned_log_path = seg_dir / "cleaned.log"
        sre_output_path = seg_dir / "sre.json"
        meta_path = seg_dir / "meta.json"

        # 1) Extract raw log bytes
        start = self.state.processed_log_bytes
        end = start + segment_log_bytes
        raw_bytes = b""
        try:
            with self.log_path.open("rb") as fp:
                fp.seek(start)
                raw_bytes = fp.read(segment_log_bytes)
        except FileNotFoundError:
            logger.warning("Log file not found while segmenting; skipping this cycle")
            return
        _write_bytes_to_file(raw_log_path, raw_bytes)

        # 2) Extract timing lines
        timing_lines_all: List[str] = []
        try:
            with self.timing_path.open("r", encoding="utf-8", errors="ignore") as tf:
                timing_lines_all = tf.read().splitlines()
        except FileNotFoundError:
            logger.warning("Timing file not found while segmenting; skipping this cycle")
            return
        start_line = self.state.processed_timing_lines
        end_line = start_line + segment_timing_lines
        timing_lines_segment = timing_lines_all[start_line:end_line]
        _write_lines_to_file(raw_timing_path, timing_lines_segment)

        # 3) Clean the log text
        try:
            cleaned_text = clean_terminal_log(raw_bytes.decode("utf-8", errors="ignore"))
        except Exception as e:
            logger.warning(f"Failed to clean log; using raw text. Error: {e}")
            cleaned_text = raw_bytes.decode("utf-8", errors="ignore")
        try:
            with cleaned_log_path.open("w", encoding="utf-8") as fp:
                fp.write(cleaned_text)
        except Exception:
            pass

        # 4) Generate SRE JSON
        try:
            with no_network():
                llm = LocalLLM(model=self.llm_model or config.ollama_model)
                service = AdviceService(llm)
                sessions = service.get_console_insights(cleaned_text)
            sre_json = json.dumps([s.model_dump() for s in sessions], indent=2)
            with sre_output_path.open("w", encoding="utf-8") as fp:
                fp.write(sre_json)
        except Exception as e:
            logger.warning(f"Failed to generate SRE for segment {seg_index}: {e}")

        # 5) Write segment metadata
        meta = {
            "segment_index": seg_index,
            "log_start_byte": start,
            "log_end_byte": end,
            "timing_start_line": start_line,
            "timing_end_line": end_line,
            "created_at": int(time.time()),
        }
        try:
            with meta_path.open("w", encoding="utf-8") as fp:
                json.dump(meta, fp, indent=2)
        except Exception:
            pass

        # 6) Advance state
        self.state.processed_log_bytes = end
        self.state.processed_timing_lines = end_line
        self.state.next_segment_index += 1
        # Be resilient to external writes that we have not yet read:
        # ensure processed_timing_lines does not exceed total lines currently present
        if self.state.processed_timing_lines > total_timing_lines_after_read:
            self.state.processed_timing_lines = total_timing_lines_after_read
        self._save_state()

        logger.debug(
            f"Archived segment {seg_index}: bytes={segment_log_bytes}, timing_lines={segment_timing_lines}"
        )

    def compact_original_files(self) -> None:
        """Physically delete processed prefixes from `.log` and `.timing` files.

        This should be invoked only after the recorder process has exited to avoid
        disrupting a writer holding open file descriptors.
        """
        try:
            pb = self.state.processed_log_bytes
            if pb > 0 and self.log_path.exists():
                tmp_path = self.log_path.with_suffix(self.log_path.suffix + ".tmp")
                with self.log_path.open("rb") as src, tmp_path.open("wb") as dst:
                    src.seek(pb)
                    while True:
                        chunk = src.read(1024 * 1024)
                        if not chunk:
                            break
                        dst.write(chunk)
                os.replace(tmp_path, self.log_path)
        except Exception as e:
            logger.warning(f"Failed to compact log file: {e}")

        try:
            tl = self.state.processed_timing_lines
            if tl > 0 and self.timing_path.exists():
                tmp_path = self.timing_path.with_suffix(self.timing_path.suffix + ".tmp")
                with self.timing_path.open("r", encoding="utf-8", errors="ignore") as src:
                    all_lines = src.read().splitlines()
                remaining = all_lines[tl:]
                with tmp_path.open("w", encoding="utf-8") as dst:
                    dst.write("\n".join(remaining) + ("\n" if remaining else ""))
                os.replace(tmp_path, self.timing_path)
        except Exception as e:
            logger.warning(f"Failed to compact timing file: {e}")


