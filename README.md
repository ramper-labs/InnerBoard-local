# InnerBoard-local

**A 100% offline onboarding reflection coach that turns private journaling into structured signals and concrete micro-advice.**

InnerBoard-local is designed for new hires who want to reflect privately on their onboarding experience while getting actionable advice. Everything runs locally on your machineâ€”no data ever leaves your device.

## ğŸ¯ What It Does

- **Private Reflection**: Write honest reflections about your onboarding experience
- **Structured Analysis**: AI extracts key points, blockers, and confidence changes from your text
- **Actionable Advice**: Get specific steps and checklists tailored to your situation
- **Zero Egress**: All processing happens offline with encrypted local storage

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  write     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   analyze   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   compose   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Journal CLI â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚ Encrypted Vault â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚  SRE (Reflection   â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚ MAC (Advice      â”‚
â”‚              â”‚            â”‚ (SQLite+Fernet) â”‚            â”‚  Extraction)       â”‚            â”‚  Composer)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚                               â”‚                               â”‚
                                     â”‚                               â–¼                               â–¼
                                     â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚ Local GGUF Model   â”‚â—€â”€â”€â”€â”€â”€â”€â–¶â”‚ Micro-Advice     â”‚
                                                          â”‚ (llama.cpp)        â”‚        â”‚ Output           â”‚
                                                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## âœ¨ Features

### ğŸ”’ **Privacy First**
- **Encrypted Storage**: All reflections encrypted with Fernet (AES-128)
- **No Network Calls**: Network guard blocks all egress during processing
- **Local AI**: Models run entirely on your machine

### ğŸ§  **Smart Processing**
- **SRE (Structured Reflection Extraction)**: Converts raw text to structured data
  - Key points extraction
  - Blocker identification with taxonomy mapping
  - Confidence delta tracking
  - Resource needs assessment
- **MAC (Micro-Advice Composer)**: Generates actionable guidance
  - Concrete steps with timeframes
  - Progress checklists
  - Urgency assessment

### âš¡ **Performance**
- **GGUF Models**: Quantized for efficient local inference
- **Metal GPU Support**: Optimized for Apple Silicon
- **Configurable**: Adjust model size, GPU layers, context window

## ğŸš€ Quick Start

### 1. Installation

```bash
git clone https://github.com/ramper-labs/InnerBoard-local.git
cd InnerBoard-local
```

**For macOS (Apple Silicon):**
```bash
CMAKE_ARGS="-DLLAMA_METAL=on" pip install llama-cpp-python
pip install -r requirements.txt
```

**For other systems:**
```bash
pip install -r requirements.txt
```

### 2. Download a Model

**Option A: Quick Test (Small Model)**
```bash
# Download a small model for testing (~400MB)
hf download bartowski/Qwen2.5-0.5B-Instruct-GGUF --include "*Q4_K_M.gguf" --local-dir ft/data
export GGUF_MODEL_PATH="ft/data/Qwen2.5-0.5B-Instruct-Q4_K_M.gguf"
```

**Option B: Production (Full Model)**
```bash
# Download the full gpt-oss-20b model (~14GB)
hf download Maziyar/gpt-oss-20b-GGUF --include "*Q5_K_M.gguf" --local-dir ft/data
export GGUF_MODEL_PATH="ft/data/gpt-oss-20b.Q5_K_M.gguf"
```

### 3. Usage

**Add a reflection and get advice:**
```bash
python -m app.main add --text "I'm struggling with the new authentication service. The documentation seems outdated and I can't get JWT validation working. Been stuck for hours."
```

**List all your reflections:**
```bash
python -m app.main list
```

## ğŸ“‹ Example Output

**Input:**
> "I spent the whole day trying to get the Kubernetes ingress to route correctly. The docs are confusing and I couldn't find good examples."

**SRE Output:**
```json
{
  "key_points": ["Ingress routing issues", "Documentation clarity problems"],
  "blockers": ["k8s ingress config", "missing examples"],
  "resources_needed": ["working ingress example", "clearer documentation"],
  "confidence_delta": -0.6
}
```

**MAC Output:**
```json
{
  "steps": [
    "Book a 20-min session with a senior engineer familiar with k8s",
    "Find working ingress examples in the team's repository",
    "Create a personal reference doc with working configs"
  ],
  "checklist": [
    "Session scheduled with k8s expert",
    "Example configs collected",
    "Personal reference created",
    "Test deployment successful"
  ],
  "urgency": "medium"
}
```

## âš™ï¸ Configuration

### Environment Variables

- `GGUF_MODEL_PATH`: Path to your GGUF model file
- `N_GPU_LAYERS`: Number of layers to offload to GPU (-1 for all, 0 for CPU-only)
- `N_CTX`: Context window size (default: 2048)

### Example Configurations

**High Performance (GPU):**
```bash
export GGUF_MODEL_PATH="ft/data/gpt-oss-20b.Q5_K_M.gguf"
export N_GPU_LAYERS=-1
export N_CTX=4096
```

**CPU Only:**
```bash
export GGUF_MODEL_PATH="ft/data/Qwen2.5-0.5B-Instruct-Q4_K_M.gguf"
export N_GPU_LAYERS=0
export N_CTX=2048
```

## ğŸ§ª Testing

Run the network safety tests:
```bash
pytest tests/test_no_network.py -v
```

Test storage encryption:
```bash
pytest tests/test_storage_crypto.py -v
```

## ğŸ“ Project Structure

```
InnerBoard-local/
â”œâ”€â”€ app/                    # Core application
â”‚   â”œâ”€â”€ main.py            # CLI interface
â”‚   â”œâ”€â”€ llm.py             # Local model loader
â”‚   â”œâ”€â”€ advice.py          # SRE + MAC orchestration
â”‚   â”œâ”€â”€ storage.py         # Encrypted vault
â”‚   â”œâ”€â”€ safety.py          # Network guard
â”‚   â””â”€â”€ models.py          # Pydantic schemas
â”œâ”€â”€ ft/                    # Fine-tuning assets
â”‚   â””â”€â”€ data/              # Model files (gitignored)
â”œâ”€â”€ tests/                 # Test suite
â”œâ”€â”€ prompts/               # System prompts
â””â”€â”€ README.md              # This file
```

## ğŸ”§ Advanced Usage

### Custom Models

You can use any GGUF model by setting the path:
```bash
export GGUF_MODEL_PATH="/path/to/your/model.gguf"
python -m app.main add --text "Your reflection"
```

### Batch Processing

Process multiple reflections from a file:
```bash
while IFS= read -r line; do
    python -m app.main add --text "$line"
done < reflections.txt
```

### Debugging

Enable verbose mode for troubleshooting:
```bash
export LLAMA_LOG_LEVEL=1
python -m app.main add --text "Your reflection"
```

## ğŸ›¡ï¸ Security Features

- **Encrypted Storage**: All reflections encrypted at rest using Fernet (AES-128)
- **Network Isolation**: Automatic network blocking during AI processing
- **Local Processing**: No data transmission to external services
- **Memory Safety**: Models run in isolated processes

## ğŸ¯ Use Cases

- **New Employee Onboarding**: Daily reflection and guidance
- **Skill Development**: Track learning progress and blockers  
- **Team Integration**: Identify social and technical challenges
- **Process Improvement**: Structured feedback for onboarding programs

## ğŸ¤ Contributing

This project follows the technical plan for the "Best Local Agent" hackathon category. Key areas for contribution:

1. **Fine-tuning**: Improve SRE accuracy with domain-specific training data
2. **Models**: Test and optimize different GGUF models
3. **UI**: Add Streamlit interface for better UX
4. **Export**: Implement privacy-preserving analytics

## ğŸ“„ License

Licensed under the Apache License 2.0. See [LICENSE](LICENSE) for details.

## ğŸ™ Acknowledgments

- Built on [llama.cpp](https://github.com/ggerganov/llama.cpp) for efficient local inference
- Uses [OpenAI's gpt-oss models](https://huggingface.co/openai/gpt-oss-20b) for reasoning
- Inspired by the need for private, offline onboarding support

---

*InnerBoard-local: Your private onboarding companion that stays on your device.*