# InnerBoard-local

**A 100% offline onboarding reflection coach that turns private journaling into structured signals and concrete micro-advice.**

InnerBoard-local is designed for new hires who want to reflect privately on their onboarding experience while getting actionable advice. Everything runs locally on your machineâ€”no data ever leaves your device.

## ğŸ¯ What It Does

- **Private Reflection**: Write honest reflections about your onboarding experience
- **Structured Analysis**: AI extracts key points, blockers, and confidence changes from your text
- **Actionable Advice**: Get specific steps and checklists tailored to your situation
- **Zero Egress**: All processing happens offline with encrypted local storage

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  write     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   analyze   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   compose   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Journal CLI â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚ Encrypted Vault â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚  SRE (Reflection   â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚ MAC (Advice      â”‚
â”‚              â”‚            â”‚ (SQLite+Fernet) â”‚            â”‚  Extraction)       â”‚            â”‚  Composer)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚                               â”‚                               â”‚
                                     â”‚                               â–¼                               â–¼
                                     â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚ Local Ollama Model â”‚â—€â”€â”€â”€â”€â”€â”€â–¶â”‚ Micro-Advice     â”‚
                                                          â”‚ (localhost:11434)  â”‚        â”‚ Output           â”‚
                                                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## âœ¨ Features

### ğŸ”’ **Privacy First**
- **Encrypted Storage**: All reflections encrypted with Fernet (AES-128)
- **No Network Calls**: Network guard blocks all egress during processing
- **Local AI**: Models run entirely on your machine

### ğŸ§  **Smart Processing**
- **SRE (Structured Reflection Extraction)**: Converts raw text to structured data
  - Key points extraction
  - Blocker identification with taxonomy mapping
  - Confidence delta tracking
  - Resource needs assessment
- **MAC (Micro-Advice Composer)**: Generates actionable guidance
  - Concrete steps with timeframes
  - Progress checklists
  - Urgency assessment

### âš¡ **Performance**
- **Ollama Local Serving**: Uses Ollama to run models locally
- **GPU Support**: Leverages your platform's capabilities via Ollama
- **Configurable**: Choose model by name and adjust Ollama host

## ğŸš€ Quick Start

### 1. Installation

```bash
git clone https://github.com/ramper-labs/InnerBoard-local.git
cd InnerBoard-local
```

**Install Python dependencies:**
```bash
pip install -r requirements.txt
```

**Install and run Ollama:**
- Download Ollama: `https://ollama.com/download`
- Ensure the server is running (default: `http://localhost:11434`).

**Pull a model (example):**
```bash
ollama pull gpt-oss:20b
```

### 2. Select a Model

The default model is `gpt-oss:20b`. To change it:
```bash
export OLLAMA_MODEL="llama3.1"
```

### 3. Usage

**Add a reflection and get advice:**
```bash
python -m app.main add --text "I'm struggling with the new authentication service. The documentation seems outdated and I can't get JWT validation working. Been stuck for hours."
```

**List all your reflections:**
```bash
python -m app.main list
```

## ğŸ“‹ Example Output

**Input:**
> "I spent the whole day trying to get the Kubernetes ingress to route correctly. The docs are confusing and I couldn't find good examples."

**SRE Output:**
```json
{
  "key_points": ["Ingress routing issues", "Documentation clarity problems"],
  "blockers": ["k8s ingress config", "missing examples"],
  "resources_needed": ["working ingress example", "clearer documentation"],
  "confidence_delta": -0.6
}
```

**MAC Output:**
```json
{
  "steps": [
    "Book a 20-min session with a senior engineer familiar with k8s",
    "Find working ingress examples in the team's repository",
    "Create a personal reference doc with working configs"
  ],
  "checklist": [
    "Session scheduled with k8s expert",
    "Example configs collected",
    "Personal reference created",
    "Test deployment successful"
  ],
  "urgency": "medium"
}
```

## âš™ï¸ Configuration

### Environment Variables

- `OLLAMA_MODEL`: Name of the model to use (default: `gpt-oss:20b`)
- `OLLAMA_HOST`: Optional custom Ollama host (default: `http://localhost:11434`)

### Example Configuration

Ollama manages CPU/GPU automatically depending on your setup.

## ğŸ§ª Testing

Run the network safety tests:
```bash
pytest tests/test_no_network.py -v
```

Test storage encryption:
```bash
pytest tests/test_storage_crypto.py -v
```

## ğŸ“ Project Structure

```
InnerBoard-local/
â”œâ”€â”€ app/                    # Core application
â”‚   â”œâ”€â”€ main.py            # CLI interface
â”‚   â”œâ”€â”€ llm.py             # Local model loader
â”‚   â”œâ”€â”€ advice.py          # SRE + MAC orchestration
â”‚   â”œâ”€â”€ storage.py         # Encrypted vault
â”‚   â”œâ”€â”€ safety.py          # Network guard
â”‚   â””â”€â”€ models.py          # Pydantic schemas
â”œâ”€â”€ ft/                    # Fine-tuning assets
â”‚   â””â”€â”€ data/              # Model files (gitignored)
â”œâ”€â”€ tests/                 # Test suite
â”œâ”€â”€ prompts/               # System prompts
â””â”€â”€ README.md              # This file
```

## ğŸ”§ Advanced Usage

### Custom Models

Use any Ollama model by name:
```bash
export OLLAMA_MODEL="llama3.1"
python -m app.main add --text "Your reflection"
```

### Batch Processing

Process multiple reflections from a file:
```bash
while IFS= read -r line; do
    python -m app.main add --text "$line"

done < reflections.txt
```

### Debugging

Enable verbose logging in your shell as needed; Ollama server logs appear in its console.

## ğŸ›¡ï¸ Security Features

- **Encrypted Storage**: All reflections encrypted at rest using Fernet (AES-128)
- **Network Isolation**: Blocks all outbound traffic during AI processing, allowing only loopback to `localhost:11434` for Ollama
- **Local Processing**: No data transmission to external services
- **Memory Safety**: Models run in isolated processes

## ğŸ¯ Use Cases

- **New Employee Onboarding**: Daily reflection and guidance
- **Skill Development**: Track learning progress and blockers  
- **Team Integration**: Identify social and technical challenges
- **Process Improvement**: Structured feedback for onboarding programs

## ğŸ¤ Contributing

This project follows the technical plan for the "Best Local Agent" hackathon category. Key areas for contribution:

1. **Fine-tuning**: Improve SRE accuracy with domain-specific training data
2. **Models**: Test and optimize different Ollama models
3. **UI**: Add Streamlit interface for better UX
4. **Export**: Implement privacy-preserving analytics

## ğŸ“„ License

Licensed under the Apache License 2.0. See [LICENSE](LICENSE) for details.

## ğŸ™ Acknowledgments

- Powered by [Ollama](https://github.com/ollama/ollama) for local model serving
- Inspired by the need for private, offline onboarding support

---

*InnerBoard-local: Your private onboarding companion that stays on your device.*